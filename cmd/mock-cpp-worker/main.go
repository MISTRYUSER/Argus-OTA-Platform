package main

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strconv"
	"syscall"
	"time"

	_ "github.com/lib/pq" // PostgreSQL driver
	"github.com/google/uuid"
	"github.com/xuewentao/argus-ota-platform/internal/domain"
	"github.com/xuewentao/argus-ota-platform/internal/infrastructure/kafka"
	"github.com/xuewentao/argus-ota-platform/internal/infrastructure/postgres"
	"github.com/xuewentao/argus-ota-platform/internal/messaging"
)

// Worker - Mock C++ Worker ç»“æ„ä½“
type Worker struct {
	kafka 		messaging.KafkaEventPublisher
	batchRepo  	domain.BatchRepository
	db			*sql.DB
}
type Config struct {
	Database DatabaseConfig
}
type DatabaseConfig struct {
	Host     string
	Port 	 int
	User     string
	Password string
	DBName   string
}
// NewWorker åˆ›å»º Worker
func NewWorker(kafka messaging.KafkaEventPublisher,batchRepo domain.BatchRepository,db *sql.DB) *Worker {
	return &Worker{
		kafka: 		kafka,
		batchRepo:  batchRepo,
		db: 		db,
	}
}

// HandleMessage å¤„ç† Kafka æ¶ˆæ¯
func (w *Worker) HandleMessage(ctx context.Context, data []byte) error {
	// 1. è§£æ JSON
	var event map[string]interface{}
	if err := json.Unmarshal(data, &event); err != nil {
		log.Printf("Failed to unmarshal event: %v", err)
		return err
	}

	eventType, ok := event["event_type"].(string)
	if !ok {
		log.Printf("Missing event_type in event")
		return fmt.Errorf("missing event_type")
	}

	// 2. äº‹ä»¶è·¯ç”±
	switch eventType {
	case "BatchCreated":
		return w.handleBatchCreated(ctx, event)

	case "StatusChanged":
		// Worker ä¸å…³å¿ƒ StatusChanged äº‹ä»¶
		return nil

	default:
		log.Printf("[Worker] Unknown event type: %s", eventType)
	}

	return nil
}
func mustAtoi(s string, field string) int  {
	i, err := strconv.Atoi(s)
    if err != nil {
        log.Fatalf("invalid %s: %s", field, s)
    }
    return i
}
// handleBatchCreated å¤„ç† BatchCreated äº‹ä»¶
// æ¨¡æ‹Ÿ C++ Worker è§£æ rec æ–‡ä»¶
func (w *Worker) handleBatchCreated(ctx context.Context, event map[string]interface{}) error {
	batchIDStr, ok := event["batch_id"].(string)
	if !ok {
		log.Printf("[Worker] Missing batch_id in event")
		return fmt.Errorf("missing batch_id")
	}

	batchID, err := uuid.Parse(batchIDStr)
	if err != nil {
		log.Printf("[Worker] Invalid batch_id: %v", err)
		return fmt.Errorf("invalid batch_id: %w", err)
	}
	batch, err := w.batchRepo.FindByID(ctx, batchID)
	if err != nil {
		return fmt.Errorf("failed to find batch: %w",err)
	}
	if batch == nil {  // â† æ·»åŠ è¿™ä¸ªæ£€æŸ¥
		return fmt.Errorf("batch not found: %s", batchID)
	}
  
  
	log.Printf("[Worker] Received BatchCreated: batch=%s", batchID)

	// æ¨¡æ‹Ÿè§£æ rec æ–‡ä»¶ï¼ˆsleep 2 ç§’ï¼‰
	log.Printf("[Worker] ğŸ”„ Simulating rec file parsing for batch %s...", batchID)
	time.Sleep(2 * time.Second)

	log.Printf("[Worker] âœ… Parsing completed for batch %s", batchID)
	fileParsedEvents := make([]domain.FileParsed,0,batch.TotalFiles)
	for i := 0;i < batch.TotalFiles;i ++{
		fileParsedEvents = append(fileParsedEvents,domain.FileParsed{
			BatchID:   batchID,
			FileID:	   uuid.New(),
			OccurredAt:time.Now(),
		})
	}
	// è½¬æ¢ä¸º DomainEvent æ¥å£ç±»å‹
	events := make([]domain.DomainEvent, len(fileParsedEvents))
	for i, e := range fileParsedEvents {
		events[i] = e
	}

	log.Printf("[Worker] Publishing %d FileParsed events...", len(events))
	if err := w.kafka.PublishEvents(ctx, events); err != nil {
		log.Printf("[Worker] Failed to publish FileParsed events: %v", err)
		return fmt.Errorf("failed to publish FileParsed events: %w", err)
	}

	log.Printf("[Worker] âœ… Successfully published %d FileParsed events for batch %s", len(events), batchID)
	return nil
}
func initDB(cfg *Config) *sql.DB {
	dsn := fmt.Sprintf(
		"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
		cfg.Database.Host,
		cfg.Database.Port,
		cfg.Database.User,
		cfg.Database.Password,
		cfg.Database.DBName,
	)
	db,err := sql.Open("postgres",dsn)
	if err != nil {
		log.Fatal("Failed to open database : ",err)
	}
	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(5)  // Idle åº”è¯¥å°äº Open
	db.SetConnMaxIdleTime(5 * time.Minute)
	db.SetConnMaxLifetime(5 * time.Minute)

	if err := db.Ping();err != nil {
		log.Fatal("Failed to ping database:", err)
	}
	log.Println("[DB] Database connected successfully")
    return db
}
func main() {
	ctx := context.Background()

	// 1. åˆå§‹åŒ– Kafka Producerï¼ˆå‘å¸ƒäº‹ä»¶ï¼‰
	kafkaProducer := initKafkaProducer()

	// 2. åˆå§‹åŒ– Kafka Consumerï¼ˆæ¶ˆè´¹äº‹ä»¶ï¼‰
	kafkaConsumer, err := kafka.NewKafkaEventConsumer(
		[]string{"localhost:9092"},
		"cpp-worker-group-v2", // Consumer Group ID (new for testing)
	)
	if err != nil {
		log.Fatalf("Failed to create Kafka consumer: %v", err)
	}
	//åˆå§‹åŒ– DB
	cfg := &Config {
		Database: DatabaseConfig{
			Host:     getEnv("DB_HOST", "localhost"),
			Port:     mustAtoi(getEnv("DB_PORT","5432"), "DB_PORT"),
			User:     getEnv("DB_USER", "argus"),
			Password: getEnv("DB_PASSWORD", "argus123"),
			DBName:   getEnv("DB_NAME", "argus_ota"),
		},
	}
	db := initDB(cfg)
	batchRepo := postgres.NewPostgresBatchRepository(db)
	// 3. åˆ›å»º Worker
	worker := NewWorker(kafkaProducer,batchRepo,db)

	// 4. å¯åŠ¨ Kafka Consumer
	topics := []string{"batch-events"}

	log.Println("========================================")
	log.Println("ğŸš€ Mock C++ Worker started successfully!")
	log.Printf("ğŸ“¡ Consuming topic: %s", topics[0])
	log.Printf("ğŸ“¦ Consumer Group: cpp-worker-group-v2")
	log.Println("========================================")

	// 5. ä¼˜é›…å…³é—­
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	// åœ¨åå° goroutine ä¸­æ¶ˆè´¹æ¶ˆæ¯
	go func() {
		if err := kafkaConsumer.Subscribe(ctx, topics, worker.HandleMessage); err != nil {
			log.Printf("Consumer error: %v", err)
		}
	}()

	// ç­‰å¾…ç³»ç»Ÿä¿¡å·
	<-sigCh
	log.Println("\nğŸ›‘ Shutting down Worker...")
	if worker.db != nil {
		if err := worker.db.Close(); err != nil {
			log.Printf("Failed to close database: %v", err)
		}
	}
	// å…³é—­ Kafka Consumer
	if err := kafkaConsumer.Close(); err != nil {
		log.Printf("Failed to close Kafka consumer: %v", err)
	}

	// å…³é—­ Kafka Producer
	if err := kafkaProducer.Close(); err != nil {
		log.Printf("Failed to close Kafka producer: %v", err)
	}

	log.Println("âœ… Worker stopped gracefully")
}

// initKafkaProducer åˆå§‹åŒ– Kafka Producer
func initKafkaProducer() messaging.KafkaEventPublisher {
	brokers := []string{getEnv("KAFKA_BROKERS", "localhost:9092")}
	topic := getEnv("KAFKA_TOPIC", "batch-events")

	producer, err := kafka.NewKafkaEventProducer(brokers, topic)
	if err != nil {
		log.Fatalf("Failed to create Kafka producer: %v", err)
	}

	return producer
}

// getEnv è¯»å–ç¯å¢ƒå˜é‡ï¼Œæä¾›é»˜è®¤å€¼
func getEnv(key, defaultValue string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return defaultValue
}
