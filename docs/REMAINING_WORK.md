# Argus OTA Platform - å‰©ä½™å·¥ä½œæ¸…å•

**æ›´æ–°æ—¥æœŸ**: 2026-01-21
**ç³»ç»Ÿå®Œæ•´åº¦**: 50%
**ç­–ç•¥**: å…ˆå®ŒæˆåŸºç¡€æ¶æ„ï¼Œeino æ”¾åˆ°æœ€å

---

## âœ… å·²å®ŒæˆåŠŸèƒ½ï¼ˆDay 1-8ï¼‰

### Domain å±‚ (70%)
- âœ… BatchStatus çŠ¶æ€æœºï¼ˆ8 ä¸ªçŠ¶æ€ + è½¬æ¢è§„åˆ™ï¼‰
- âœ… ProcessingStatus çŠ¶æ€æœºï¼ˆ5 ä¸ªçŠ¶æ€ + è½¬æ¢è§„åˆ™ï¼‰
- âœ… Batch èšåˆæ ¹ï¼ˆ6 ä¸ªæ–¹æ³•ï¼‰
- âœ… File èšåˆæ ¹ï¼ˆåŸºç¡€ç»“æ„ï¼‰
- âœ… é¢†åŸŸäº‹ä»¶ï¼ˆBatchCreated, StatusChanged, FileParsedï¼‰

### Application å±‚ (70%)
- âœ… BatchServiceï¼ˆCreateBatch, TransitionBatchStatus, AddFileï¼‰
- âœ… OrchestrateServiceï¼ˆäº‹ä»¶è·¯ç”±ã€çŠ¶æ€æœºã€Redis Barrierï¼‰
- â¬œ QueryServiceï¼ˆæŠ¥å‘ŠæŸ¥è¯¢ + Singleflightï¼‰

### Infrastructure å±‚ (75%)
- âœ… PostgreSQL Repositoryï¼ˆ5 ä¸ªæ–¹æ³•ï¼‰
- âœ… Redis Clientï¼ˆ7 ä¸ªæ–¹æ³• + Pipelineï¼‰
- âœ… Kafka Producerï¼ˆ3 ä¸ªäº‹ä»¶ç±»å‹æ”¯æŒï¼‰
- âœ… Kafka Consumerï¼ˆConsumer Group + æ‰‹åŠ¨æäº¤ï¼‰
- âœ… MinIO Clientï¼ˆæµå¼ä¸Šä¼ ï¼‰

### Interfaces å±‚ (45%)
- âœ… HTTP Handlerï¼ˆCreateBatch, UploadFile, CompleteUploadï¼‰
- â¬œ Query Handlerï¼ˆæŠ¥å‘ŠæŸ¥è¯¢ï¼‰
- â¬œ SSE Handlerï¼ˆåŸºç¡€å®ç°ï¼Œåç»­è¿ç§»åˆ° einoï¼‰

### cmd/ (70%)
- âœ… cmd/ingestor/main.goï¼ˆå®Œæ•´å®ç°ï¼‰
- âœ… cmd/orchestrator/main.goï¼ˆå®Œæ•´å®ç°ï¼‰
- âœ… cmd/mock-cpp-worker/main.goï¼ˆå®Œæ•´å®ç°ï¼‰
- â¬œ cmd/query-service/main.go

### æµ‹è¯•éªŒè¯ (60%)
- âœ… Worker å•å…ƒæµ‹è¯•ï¼ˆ10/10 FileParsed äº‹ä»¶ï¼‰
- âœ… å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆ6/6 æ ¸å¿ƒæ­¥éª¤ï¼‰
- âœ… Redis Barrier éªŒè¯ï¼ˆSADD + SCARDï¼‰
- â¬œ ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•

---

## ğŸ”¥ é«˜ä¼˜å…ˆçº§å‰©ä½™å·¥ä½œï¼ˆDay 9-12ï¼‰

### 1. Query Service + Singleflight (2 å¤©)

**ç›®æ ‡**: å®ç°é«˜å¹¶å‘æŠ¥å‘ŠæŸ¥è¯¢ï¼Œé˜²æ­¢ç¼“å­˜å‡»ç©¿

**ä»»åŠ¡**:
- [ ] å®ç° `internal/application/query_service.go`
  ```go
  type QueryService struct {
      sf       singleflight.Group
      repo     ReportRepository
      cache    CacheClient
  }

  func (s *QueryService) GetReport(ctx context.Context, batchID uuid.UUID) (*Report, error) {
      result, err, _ := s.sf.Do(batchID.String(), func() (interface{}, error) {
          // 1. æŸ¥è¯¢ç¼“å­˜
          report, err := s.cache.Get(ctx, batchID)
          if err == nil {
              return report, nil
          }

          // 2. æŸ¥è¯¢æ•°æ®åº“
          report, err = s.repo.FindByID(ctx, batchID)
          if err != nil {
              return nil, err
          }

          // 3. å†™å…¥ç¼“å­˜
          s.cache.Set(ctx, batchID, report, 10*time.Minute)
          return report, nil
      })

      return result.(*Report), err
  }
  ```

- [ ] å®ç° `internal/interfaces/http/handlers/query_handler.go`
  - `GET /api/v1/batches/:id/report` - æŠ¥å‘ŠæŸ¥è¯¢
  - `GET /api/v1/batches/:id/progress` - è¿›åº¦æŸ¥è¯¢

- [ ] å®ç° `cmd/query-service/main.go`
  - ä¾èµ–æ³¨å…¥ï¼šConfig â†’ Redis â†’ PostgreSQL â†’ QueryService â†’ Handler
  - ä¼˜é›…å…³é—­ï¼šSIGINT/SIGTERM

**éªŒè¯ç›®æ ‡**:
- 100 å¹¶å‘æŸ¥è¯¢ â†’ 1 æ¬¡æ•°æ®åº“æŸ¥è¯¢
- ç¼“å­˜å‘½ä¸­ç‡ > 90%

**é¢è¯•è€ƒç‚¹**:
- **Q: ä»€ä¹ˆæ˜¯ç¼“å­˜å‡»ç©¿ï¼Ÿ**
  - A: çƒ­ç‚¹ key è¿‡æœŸï¼Œå¤§é‡å¹¶å‘ç›´æ¥æ‰“åˆ°æ•°æ®åº“
- **Q: Singleflight å¦‚ä½•è§£å†³ï¼Ÿ**
  - A: ç›¸åŒ key çš„å¹¶å‘è¯·æ±‚åˆå¹¶ä¸º 1 æ¬¡

---

### 2. ä¿®å¤çŠ¶æ€è½¬æ¢æµç¨‹ (1-2 å¤©)

**é—®é¢˜**: å½“å‰ TotalFiles = 0ï¼ŒçŠ¶æ€è½¬æ¢å¡åœ¨ scattering

**è§£å†³æ–¹æ¡ˆ**:
- [ ] **æ­¥éª¤ 1**: ä¿®æ”¹ Workerï¼Œä»æ•°æ®åº“æŸ¥è¯¢ TotalFiles
  ```go
  func (w *Worker) handleBatchCreated(ctx context.Context, event map[string]interface{}) error {
      batchIDStr := event["batch_id"].(string)
      batchID, _ := uuid.Parse(batchIDStr)

      // æŸ¥è¯¢ Batch TotalFiles
      batch, err := w.batchRepo.FindByID(ctx, batchID)
      if err != nil {
          return fmt.Errorf("failed to find batch: %w", err)
      }

      // å‘å¸ƒå¯¹åº”æ•°é‡çš„ FileParsed äº‹ä»¶
      var events []domain.DomainEvent
      for i := 0; i < batch.TotalFiles; i++ {
          events = append(events, domain.FileParsed{
              BatchID:   batchID,
              FileID:    uuid.New(),
              OccurredAt: time.Now(),
          })
      }

      return w.kafka.PublishEvents(ctx, events)
  }
  ```

- [ ] **æ­¥éª¤ 2**: Worker æ³¨å…¥ BatchRepository
  ```go
  type Worker struct {
      kafka messaging.KafkaEventPublisher
      batchRepo domain.BatchRepository  // æ–°å¢
  }

  func NewWorker(kafka messaging.KafkaEventPublisher, batchRepo domain.BatchRepository) *Worker {
      return &Worker{
          kafka:     kafka,
          batchRepo: batchRepo,
      }
  }
  ```

- [ ] **æ­¥éª¤ 3**: ä¿®æ”¹ Worker main.go
  ```go
  func main() {
      // ... åˆå§‹åŒ– PostgreSQL
      db := initDB()

      // åˆå§‹åŒ– BatchRepository
      batchRepo := postgres.NewBatchRepository(db)

      // åˆ›å»º Workerï¼ˆæ³¨å…¥ Repositoryï¼‰
      worker := NewWorker(kafkaProducer, batchRepo)

      // ... å¯åŠ¨ Worker
  }
  ```

- [ ] **æ­¥éª¤ 4**: éªŒè¯å®Œæ•´æµç¨‹
  - scattering â†’ scattered (æ‰€æœ‰æ–‡ä»¶è§£æå®Œæˆ)
  - scattered â†’ gathering (è§¦å‘ä¸‹ä¸€æ­¥)
  - gathering â†’ gathered (èšåˆå®Œæˆ)
  - gathered â†’ diagnosing (è§¦å‘ AI)
  - diagnosing â†’ completed (è¯Šæ–­å®Œæˆ)

---

### 3. ç«¯åˆ°ç«¯é›†æˆæµ‹è¯• (1-2 å¤©)

**ç›®æ ‡**: éªŒè¯å®Œæ•´æµç¨‹ä»å¤´åˆ°å°¾

**ä»»åŠ¡**:
- [ ] åˆ›å»º `tests/e2e/full_flow_test.go`
  ```go
  func TestFullFlow(t *testing.T) {
      // 1. åˆ›å»º Batch
      batch := createBatch(t, "TEST-001", "TESTVIN001")

      // 2. ä¸Šä¼ æ–‡ä»¶ï¼ˆé€šè¿‡ APIï¼‰
      uploadFile(t, batch.ID, "test.log")
      uploadFile(t, batch.ID, "test2.log")

      // 3. å®Œæˆä¸Šä¼ 
      completeUpload(t, batch.ID)

      // 4. ç­‰å¾…çŠ¶æ€è½¬æ¢ï¼ˆä½¿ç”¨è½®è¯¢ï¼‰
      waitForStatus(t, batch.ID, "pending", 1*time.Second)
      waitForStatus(t, batch.ID, "uploaded", 1*time.Second)
      waitForStatus(t, batch.ID, "scattering", 1*time.Second)
      waitForStatus(t, batch.ID, "scattered", 10*time.Second)  // Worker å¤„ç†éœ€è¦æ—¶é—´
      waitForStatus(t, batch.ID, "gathering", 1*time.Second)
      waitForStatus(t, batch.ID, "gathered", 5*time.Second)

      // 5. æŸ¥è¯¢æŠ¥å‘Š
      report := getReport(t, batch.ID)
      assert.NotNil(t, report)
      assert.Equal(t, batch.ID, report.BatchID)
  }
  ```

- [ ] æ€§èƒ½æµ‹è¯•
  ```go
  func TestConcurrentUpload(t *testing.T) {
      // 100 ä¸ªå¹¶å‘ Batch ä¸Šä¼ 
      var wg sync.WaitGroup
      for i := 0; i < 100; i++ {
          wg.Add(1)
          go func(idx int) {
              defer wg.Done()
              createBatch(t, fmt.Sprintf("CONCURRENT-%d", idx))
          }(i)
      }
      wg.Wait()
  }
  ```

- [ ] æ•…éšœæ¢å¤æµ‹è¯•
  - Worker å´©æºƒæ¢å¤
  - Kafka æ¶ˆæ¯é‡å¤æ¶ˆè´¹ï¼ˆéªŒè¯å¹‚ç­‰æ€§ï¼‰
  - Redis è¿æ¥æ–­å¼€é‡è¿

---

## ğŸ“… ä¸­ä¼˜å…ˆçº§å·¥ä½œï¼ˆDay 13-15ï¼‰

### 4. SSE å®æ—¶è¿›åº¦æ¨é€ï¼ˆåŸºç¡€å®ç°ï¼‰(1-2 å¤©)

**ç›®æ ‡**: å…ˆå®ç°åŸºç¡€ SSEï¼Œåç»­è¿ç§»åˆ° eino

**ä»»åŠ¡**:
- [ ] å®ç° Redis Pub/Sub è¿›åº¦å¹¿æ’­
  ```go
  func (s *OrchestrateService) PublishProgress(ctx context.Context, batchID uuid.UUID, progress int) {
      channel := fmt.Sprintf("batch:%s:progress", batchID)
      message := fmt.Sprintf(`{"batch_id":"%s","progress":%d}`, batchID, progress)
      s.redis.Publish(ctx, channel, message)
  }
  ```

- [ ] å®ç° SSE Handler
  ```go
  func (h *SSEHandler) StreamProgress(c *gin.Context) {
      batchID := c.Param("id")

      // è®¾ç½® SSE å“åº”å¤´
      c.Writer.Header().Set("Content-Type", "text/event-stream")
      c.Writer.Header().Set("Cache-Control", "no-cache")
      c.Writer.Header().Set("Connection", "keep-alive")

      // è®¢é˜… Redis Pub/Sub
      pubsub := redisClient.Subscribe(ctx, fmt.Sprintf("batch:%s:progress", batchID))
      defer pubsub.Close()

      // æµå¼æ¨é€
      for {
          msg, err := pubsub.ReceiveMessage(ctx)
          if err != nil {
              break
          }

          fmt.Fprintf(c.Writer, "data: %s\n\n", msg.Payload)
          c.Writer.Flush()
      }
  }
  ```

- [ ] æ·»åŠ è¿›åº¦å¹¿æ’­ç‚¹
  - FileParsed äº‹ä»¶å¤„ç†æ—¶ï¼šPublishProgress(batchID, processedCount)
  - çŠ¶æ€è½¬æ¢æ—¶ï¼šPublishProgress(batchID, newStatus)

**æ³¨æ„**: è¿™æ˜¯ä¸´æ—¶å®ç°ï¼Œåç»­ä¼šè¿ç§»åˆ° eino

---

### 5. Python Worker å®ç° (2-3 å¤©)

**ç›®æ ‡**: å®ç° Gather é˜¶æ®µçš„æ•°æ®èšåˆ

**ä»»åŠ¡**:
- [ ] åˆ›å»º `workers/python-aggregator/main.py`
  ```python
  from confluent_kafka import Consumer, Producer
  from minio import Minio
  import pandas as pd

  def main():
      # Kafka Consumer
      consumer = Consumer({
          'bootstrap.servers': 'localhost:9092',
          'group.id': 'python-aggregator-group',
          'auto.offset.reset': 'earliest'
      })
      consumer.subscribe(['batch-events'])

      # MinIO Client
      minio = Minio('localhost:9000', ...)

      # Kafka Producer
      producer = Producer({'bootstrap.servers': 'localhost:9092'})

      while True:
          msg = consumer.poll(1.0)
          if msg is None:
              continue

          event = json.loads(msg.value())
          if event['event_type'] == 'AllFilesScattered':
              # èšåˆæ•°æ®
              aggregate_data(event['batch_id'])
              # å‘å¸ƒäº‹ä»¶
              producer.produce('batch-events', json.dumps({
                  'event_type': 'AllFilesGathered',
                  'batch_id': event['batch_id'],
                  ...
              }))
  ```

- [ ] å®ç° AllFilesGathered äº‹ä»¶
  ```go
  type AllFilesGathered struct {
      BatchID     uuid.UUID
      TotalLines  int64
      ErrorCodes  map[string]int
      OccurredAt  time.Time
  }
  ```

---

## ğŸš€ ä½ä¼˜å…ˆçº§å·¥ä½œï¼ˆDay 16+ï¼‰

### 6. AI Diagnose (ä½¿ç”¨ eino) (2-3 å¤©)

**ä»»åŠ¡**:
- [ ] å®‰è£… eino
  ```bash
  go get github.com/cloudwego/eino
  ```

- [ ] ä½¿ç”¨ eino LLM API
  ```go
  import "github.com/cloudwego/eino/components/model/openai"

  llm := openai.NewLLM(openai.Config{
      APIKey: os.Getenv("OPENAI_API_KEY"),
  })

  prompt := eino.NewPrompt(
      "ä½ æ˜¯ OTA æ—¥å¿—è¯Šæ–­ä¸“å®¶",
      "åˆ†æä»¥ä¸‹æ•°æ®...",
  )

  response, err := llm.Generate(ctx, prompt)
  ```

- [ ] è¿ç§» SSE åˆ° eino
  ```go
  // æ›¿æ¢ç°æœ‰çš„ SSE å®ç°
  stream := eino.NewStream()
  stream.Write(...)
  ```

---

### 7. ç›‘æ§ä¸è¿ç»´ (1-2 å¤©)

**ä»»åŠ¡**:
- [ ] Prometheus metrics
  ```go
  import "github.com/prometheus/client_golang/prometheus"

  var (
      batchCreatedTotal = prometheus.NewCounter(...)
      fileProcessedTotal = prometheus.NewCounter(...)
      kafkaConsumerLag = prometheus.NewGauge(...)
  )
  ```

- [ ] Docker Compose ç”Ÿäº§é…ç½®
  - èµ„æºé™åˆ¶
  - å¥åº·æ£€æŸ¥
  - æ—¥å¿—è½®è½¬

---

## ğŸ“Š å·¥ä½œé‡è¯„ä¼°

| æ¨¡å— | å·¥ä½œé‡ | ä¼˜å…ˆçº§ | å¤‡æ³¨ |
|------|--------|--------|------|
| Query Service + Singleflight | 2 å¤© | ğŸ”¥ é«˜ | é˜²ç¼“å­˜å‡»ç©¿ |
| çŠ¶æ€è½¬æ¢ä¿®å¤ | 1-2 å¤© | ğŸ”¥ é«˜ | TotalFiles æŸ¥è¯¢ |
| ç«¯åˆ°ç«¯æµ‹è¯• | 1-2 å¤© | ğŸ”¥ é«˜ | è´¨é‡ä¿è¯ |
| SSE åŸºç¡€å®ç° | 1-2 å¤© | ğŸ“… ä¸­ | ä¸´æ—¶å®ç° |
| Python Worker | 2-3 å¤© | ğŸ“… ä¸­ | Gather é˜¶æ®µ |
| AI Diagnose (eino) | 2-3 å¤© | ğŸš€ ä½ | æœ€ååš |
| ç›‘æ§è¿ç»´ | 1-2 å¤© | ğŸš€ ä½ | ç”Ÿäº§å°±ç»ª |

**æ€»å·¥ä½œé‡**: 10-14 å¤©

---

## ğŸ¯ æ›´æ–°çš„é‡Œç¨‹ç¢‘

- [x] **Milestone 1**: åŸºç¡€æ¶æ„å®Œæˆï¼ˆDay 1-7ï¼‰âœ…
- [x] **Milestone 2**: Worker å®ç°ä¸æµ‹è¯•ï¼ˆDay 8ï¼‰âœ…
- [ ] **Milestone 3**: æŸ¥è¯¢ + çŠ¶æ€è½¬æ¢ä¿®å¤ï¼ˆDay 9-12ï¼‰
- [ ] **Milestone 4**: Gather é˜¶æ®µ + AIï¼ˆDay 13-16ï¼‰
- [ ] **Milestone 5**: ç”Ÿäº§å°±ç»ªï¼ˆDay 17+ï¼‰

---

## ğŸ“ æŠ€æœ¯å€ºåŠ¡

### éœ€è¦æ”¹è¿›çš„åœ°æ–¹

1. **Worker TotalFiles æŸ¥è¯¢** ğŸ”¥
   - å½“å‰ï¼šç¡¬ç¼–ç  2 ä¸ªæ–‡ä»¶
   - æ”¹è¿›ï¼šä»æ•°æ®åº“æŸ¥è¯¢çœŸå® TotalFiles
   - ä¼˜å…ˆçº§ï¼šé«˜

2. **çŠ¶æ€è½¬æ¢å®Œæˆ** ğŸ”¥
   - å½“å‰ï¼šå¡åœ¨ scattering
   - æ”¹è¿›ï¼šå®Œæˆæ‰€æœ‰çŠ¶æ€è½¬æ¢
   - ä¼˜å…ˆçº§ï¼šé«˜

3. **å•å…ƒæµ‹è¯•è¦†ç›–**
   - å½“å‰ï¼šåªæœ‰ BatchService æµ‹è¯•
   - æ”¹è¿›ï¼šæ·»åŠ  OrchestrateServiceã€QueryService æµ‹è¯•
   - ä¼˜å…ˆçº§ï¼šä¸­

4. **ç›‘æ§ä¸æ—¥å¿—**
   - å½“å‰ï¼šåªæœ‰åŸºç¡€æ—¥å¿—
   - æ”¹è¿›ï¼šæ·»åŠ  Prometheus metrics
   - ä¼˜å…ˆçº§ï¼šä½

---

**å¤‡æ³¨**:
- å½“å‰ç³»ç»Ÿå®Œæ•´åº¦ï¼š50%
- æ ¸å¿ƒæ¶æ„å·²éªŒè¯ï¼šâœ…
- ç­–ç•¥ï¼šå…ˆå®ŒæˆåŸºç¡€æ¶æ„ï¼Œeino æ”¾åˆ°æœ€å
- é¢„è®¡å®Œæˆæ—¶é—´ï¼š10-14 å¤©
